{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplot parameters\n",
    "plt.rcParams.update({\n",
    "    \"animation.writer\": \"ffmpeg\",\n",
    "    \"font.family\": \"serif\",  # use serif/main font for text elements\n",
    "    \"font.size\": 12,\n",
    "    \"text.usetex\": True,     # use inline math for ticks\n",
    "    \"pgf.rcfonts\": False,    # don't setup fonts from rc parameters\n",
    "    \"hist.bins\": 20, # default number of bins in histograms\n",
    "    \"pgf.preamble\": [\n",
    "         \"\\\\usepackage{units}\",          # load additional packages\n",
    "         \"\\\\usepackage{metalogo}\",\n",
    "         \"\\\\usepackage{unicode-math}\",   # unicode math setup\n",
    "         r\"\\setmathfont{xits-math.otf}\",\n",
    "         r\"\\setmainfont{DejaVu Serif}\",  # serif font via preamble\n",
    "         r'\\usepackage{color}',\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Data\n",
    "## no convergence example\n",
    "x = torch.Tensor(np.array([-0.75,-0.5,0.125]).reshape(-1,1)) \n",
    "y = torch.Tensor(np.array([1.1,0.1,0.8]).reshape(-1,1)) \n",
    "\n",
    "## Generate Stewart et al. data\n",
    "x_stew = np.array([-1, -0.7, -0.55, -0.4, 0, 0.5, 0.6, 0.7, 1])\n",
    "y_stew = np.array([0, 0.7, 0.5, 1, 0, 1, 0.6, 0.7, 0])\n",
    "x = torch.Tensor(np.random.uniform(-1, 1, 40).reshape(-1,1))\n",
    "y = torch.Tensor(np.interp(x, x_stew, y_stew))\n",
    "\n",
    "#############################\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# view data\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"tab:green\")\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.grid(alpha=0.2)\n",
    "#plt.savefig(\"example.pdf\")\n",
    "#plt.ylim(0,1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural network architecture and initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    1 hidden layer Relu network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output, init_scale=1, bias_hidden=True, initialisation='balanced', **kwargs):\n",
    "        \"\"\"\n",
    "        n_feature: dimension of input\n",
    "        n_hidden: number of hidden neurons\n",
    "        n_output: dimension of output\n",
    "        init_scale: all the weights are initialized ~ N(0, init_scale^2/m) where m is the input dimension of this layer\n",
    "        bias_hidden: if True, use bias parameters in hidden layer. Use no bias otherwise\n",
    "        bias_output: if True, use bias parameters in output layer. Use no bias otherwise\n",
    "        intialisation: 'balanced', 'unbalanced' or 'dominated'\n",
    "                            - balanced ensures ||w_j|| = |a_j|\n",
    "                            - unbalanced ensures no relation and independently initialise gaussian weights\n",
    "                            - dominated ensures |a_j| > ||w_j||\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.init_scale = init_scale/np.sqrt(n_hidden) # normalisation by sqrt(m)\n",
    "        self.initialisation_ = initialisation\n",
    "        \n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden, bias=bias_hidden)   # hidden layer with rescaled init\n",
    "\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output, bias=False)   # output layer with rescaled init\n",
    "        \n",
    "        if initialisation=='balanced': # balanced initialisation\n",
    "            torch.nn.init.normal_(self.hidden.weight.data, std=self.init_scale)\n",
    "            if bias_hidden:\n",
    "                torch.nn.init.normal_(self.hidden.bias.data, std=self.init_scale)\n",
    "                neuron_norms = (self.hidden.weight.data.norm(dim=1).square()+self.hidden.bias.data.square()).sqrt()\n",
    "            else:\n",
    "                neuron_norms = (self.hidden.weight.data.norm(dim=1).square()).sqrt()\n",
    "            self.predict.weight.data = 2*torch.bernoulli(0.5*torch.ones_like(self.predict.weight.data)) -1\n",
    "            self.predict.weight.data *= neuron_norms\n",
    "            \n",
    "        if initialisation=='unbalanced':\n",
    "            torch.nn.init.normal_(self.hidden.weight.data, std=self.init_scale)\n",
    "            if bias_hidden:\n",
    "                torch.nn.init.normal_(self.hidden.bias.data, std=self.init_scale)\n",
    "            torch.nn.init.normal_(self.predict.weight.data, std=self.init_scale)\n",
    "            \n",
    "        if initialisation=='dominated':\n",
    "            torch.nn.init.uniform_(self.hidden.weight.data, a=-self.init_scale, b=self.init_scale)\n",
    "            self.predict.weight.data = 2*torch.bernoulli(0.5*torch.ones_like(self.predict.weight.data)) -1\n",
    "            self.predict.weight.data *= self.init_scale\n",
    "            if bias_hidden:\n",
    "                torch.nn.init.uniform_(self.hidden.bias.data, a=-self.init_scale, b=self.init_scale)\n",
    "                self.predict.weight.data *= np.sqrt(2)\n",
    "            \n",
    "        self.activation = kwargs.get('activation', torch.nn.ReLU()) # activation of hidden layer\n",
    "        \n",
    "        if kwargs.get('zero_output', False):\n",
    "            # ensure that the estimated function is 0 at initialisation\n",
    "            # useful when initialising in lazy regime\n",
    "            half_n = int(n_hidden/2)\n",
    "            self.hidden.weight.data[half_n:] = self.hidden.weight.data[:half_n]\n",
    "            if bias_hidden:\n",
    "                self.hidden.bias.data[half_n:] = self.hidden.bias.data[:half_n]\n",
    "            self.predict.weight.data[0, half_n:] = -self.predict.weight.data[0, :half_n]\n",
    "            \n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.activation(self.hidden(z))     \n",
    "        z = self.predict(z)             # linear output\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicolor_label(ax,list_of_strings,list_of_colors,axis='x',anchorpad=0,**kw):\n",
    "    \"\"\"this function creates axes labels with multiple colors\n",
    "    ax: specifies the axes object where the labels should be drawn\n",
    "    list_of_strings: a list of all of the text items\n",
    "    list_if_colors: a corresponding list of colors for the strings\n",
    "    axis:'x', 'y', or 'both' and specifies which label(s) should be drawn\"\"\"\n",
    "    from matplotlib.offsetbox import AnchoredOffsetbox, TextArea, HPacker, VPacker\n",
    "\n",
    "    # x-axis label\n",
    "    if axis=='x' or axis=='both':\n",
    "        boxes = [TextArea(text, textprops=dict(color=color, ha='left',va='bottom',**kw)) \n",
    "                    for text,color in zip(list_of_strings,list_of_colors) ]\n",
    "        xbox = HPacker(children=boxes,align=\"center\",pad=0, sep=60)\n",
    "        anchored_xbox = AnchoredOffsetbox(loc=3, child=xbox, pad=anchorpad,frameon=False,bbox_to_anchor=(0.27, -0.18),\n",
    "                                          bbox_transform=ax.transAxes, borderpad=0.)\n",
    "        ax.add_artist(anchored_xbox)\n",
    "\n",
    "    # y-axis label\n",
    "    if axis=='y' or axis=='both':\n",
    "        boxes = [TextArea(text, textprops=dict(color=color, ha='left',va='bottom',rotation=90,**kw)) \n",
    "                     for text,color in zip(list_of_strings[::-1],list_of_colors) ]\n",
    "        ybox = VPacker(children=boxes,align=\"center\", pad=0, sep=5)\n",
    "        anchored_ybox = AnchoredOffsetbox(loc=3, child=ybox, pad=anchorpad, frameon=False, bbox_to_anchor=(-0.10, 0.2), \n",
    "                                          bbox_transform=ax.transAxes, borderpad=0.)\n",
    "        ax.add_artist(anchored_ybox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_frame(fig, arts, frame_number, name=\"frame\", ext=\"pdf\", **kwargs):\n",
    "    \"\"\"save a single frame of an animation\n",
    "    fig: the figure to save\n",
    "    arts: list of images resulting in the animation\n",
    "    frame_number: the specific frame to save as a pdf\n",
    "    ext: extension of the file (pdf, png, svg...). Has to be compatible with matplotlib savefig format\n",
    "    kwargs: additional arguments to pass to matplotlib.savefig (e.g., dpi)\n",
    "    \"\"\"\n",
    "    # make sure everything is hidden\n",
    "    for frame_arts in arts:\n",
    "        for art in frame_arts:\n",
    "            art.set_visible(False)\n",
    "    # make the one artist we want visible\n",
    "    for i in range(len(arts[frame_number])):\n",
    "        arts[frame_number][i].set_visible(True)\n",
    "    fig.savefig(name+\"_{}.{}\".format(frame_number,ext), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for experiments\n",
    "params = {'n_hidden': 200000,\n",
    "         'n_iterations':2000000,\n",
    "         'init_scale':1e-3,\n",
    "         'initialisation':'unbalanced',\n",
    "         'learning_rate':0.001}\n",
    "\n",
    "# init network\n",
    "net = Net(n_feature=1, n_output=1, bias_hidden=True, **params)     # define the network\n",
    " \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=params['learning_rate']) #Gradient descent\n",
    "loss_func = torch.nn.MSELoss(reduction='mean')  # mean squared error\n",
    "\n",
    "n_samples = x.shape[0]\n",
    "\n",
    "loss = torch.Tensor(np.array([0]))\n",
    "previous_loss = torch.Tensor(np.array([np.infty]))\n",
    "\n",
    "losses = []\n",
    "\n",
    "# plot parameters\n",
    "iter_geom = 1.1 #saved frames correspond to steps t=\\lceil k^{iter_geom} \\rceil for all integers k \n",
    "last_iter = 0\n",
    "frame = 0\n",
    "nets = []\n",
    "iters = []\n",
    "\n",
    "# train the network\n",
    "for it in tqdm(range(params['n_iterations'])):\n",
    "    prediction = net(x)\n",
    "    loss = loss_func(prediction, y) \n",
    "    if (it<2 or it==int(last_iter*iter_geom)+1): # save net weights\n",
    "        nets.append(copy.deepcopy(net))\n",
    "        iters.append(it)\n",
    "        last_iter = it\n",
    "    losses.append(loss.data.numpy())\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # descent step\n",
    "    \n",
    "# save last iterate\n",
    "nets.append(copy.deepcopy(net))\n",
    "iters.append(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load from $\\texttt{saves}$ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the experiments from a terminal instead of a notebook, the $\\texttt{saves}$ folder is used to save all intermediate networks weights. It is especially useful for long simulations (e.g. generating Figure 3 in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_hidden': 200000}\n",
    "net = Net(n_feature=1, n_output=1, bias_hidden=True, **params)\n",
    "\n",
    "x = torch.load('saves/x.pth')\n",
    "y = torch.load('saves/y.pth')\n",
    "\n",
    "iters = np.loadtxt('saves/iters.txt', dtype=\"int\")\n",
    "losses = np.loadtxt('saves/losses.txt')\n",
    "nets = []\n",
    "\n",
    "with open(\"saves/nets.pth\",\"rb\") as netfile:\n",
    "    while True: \n",
    "        try:\n",
    "            net.load_state_dict(pickle.load(netfile))\n",
    "            nets.append(copy.deepcopy(net))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evolution of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "fig = plt.figure(\"Neuron alignment\")\n",
    "plt.ioff()\n",
    "\n",
    "# Cosmetics\n",
    "c1 = 'tab:green' # color of left axis\n",
    "c2 = 'tab:blue' # color of right axis\n",
    "c3 = 'black'\n",
    "color_map = {0 : 'firebrick',\n",
    "             0.5 : 'black',\n",
    "             1 : 'darkviolet'}\n",
    "\n",
    "#plt.subplots_adjust(left=0.15, right=0.85)\n",
    "\n",
    "ax = fig.add_subplot(111, projection='polar') # polar coordinates\n",
    "ax.set_rorigin(-5e-2) # set inner circle for 0 norm vectors\n",
    "ax.set_theta_zero_location(\"E\")\n",
    "ax.yaxis.set_ticklabels([])\n",
    "\n",
    "##plot vlines for sector delimitations (omitted for Figure 3 plot)\n",
    "#w = net.hidden.weight.data.detach().numpy().copy().reshape(-1)\n",
    "#b = net.hidden.bias.data.detach().numpy().copy()\n",
    "#ax.vlines(np.arctan(1/x)+np.pi/2, 0, np.max(np.sqrt(w**2+b**2)), colors=c1, linestyles='dashed', lw=1)\n",
    "#ax.vlines(np.arctan(1/x)-np.pi/2, 0, np.max(np.sqrt(w**2+b**2)), colors=c1, linestyles='dashed', lw=1)\n",
    "\n",
    "##plot point for x_i (omitted for Figure 3 plot)\n",
    "#ax.scatter(np.arctan(1/x)+np.pi*np.heaviside(-x,0), np.max(np.sqrt(w**2+b**2))*np.ones_like(x), c=c3, marker='+')\n",
    "#for i, xi in enumerate(x):\n",
    "    #ax.annotate(r'$x_{{{n}}}$'.format(n=i+1), (np.arctan(1/xi)+np.pi*np.heaviside(-xi,0), np.max(np.sqrt(w**2+b**2))),\n",
    "                #xytext=(2.5, -2), textcoords='offset points')\n",
    "\n",
    "#######\n",
    "for i,net in enumerate(nets):\n",
    "    w = net.hidden.weight.data.detach().numpy().reshape(-1)\n",
    "    s = net.predict.weight.data.heaviside(torch.as_tensor(float(0.5))).reshape(-1).numpy().copy()\n",
    "    b = net.hidden.bias.data.detach().numpy()\n",
    "    it = iters[i]\n",
    "    c = [color_map[d] for d in s] # color of stars given their output layer sign\n",
    "    im = ax.scatter(np.arctan(b/w)+np.pi*np.heaviside(-w,0), np.sqrt(w**2+b**2), animated=True, c=c, marker='*')\n",
    "    t1 = ax.annotate(\"iteration: \"+str(it),(0.75,0.95),xycoords='figure fraction',annotation_clip=False) # add text\n",
    "    ims.append([im,t1])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig, ims, interval=100, repeat=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('neuron_alignment.mp4', fps=10, dpi=120) # save animation as video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save specific frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/opt/anaconda3/lib/python3.9/site-packages/matplotlib/animation.py:889: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you have outputted the Animation using `plt.show()` or `anim.save()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "del ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(iters, np.ndarray):\n",
    "    iters = iters.tolist()\n",
    "#it_to_save = iters[0]\n",
    "it_to_save = 1999999\n",
    "frame = iters.index(it_to_save)\n",
    "save_single_frame(fig, ims, frame, name=\"frame_alignment\", ext=\"png\", dpi=600) # save specific frame of animation as a pdf or png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "fig = plt.figure(\"Estimated function\")\n",
    "plt.ioff()\n",
    "\n",
    "# Cosmetics\n",
    "c1 = 'tab:green' # color of left axis\n",
    "c2 = 'tab:blue' # color of right axis\n",
    "c3 = 'black'\n",
    "c4 = 'tab:red'\n",
    "plot_OLS = False # if True, also plot the OLS estimator\n",
    "plot_stew = True # if True, also plot the teacher estimator (data of Stewart et al.)\n",
    "\n",
    "plt.subplots_adjust(left=0.15, right=0.85, bottom=0.15)\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xlim(-1,1)\n",
    "#ax1.set_ylim(-0.5,1.5)\n",
    "ax1.set_ylabel(r'$y$', fontsize=20)\n",
    "ax1.set_xlabel(r'$x$', fontsize=20)\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "\n",
    "#######\n",
    "z = torch.Tensor(np.linspace(-1,1,100).reshape(-1,1))\n",
    "\n",
    "for i,net in enumerate(nets):\n",
    "    it = iters[i]\n",
    "    im1, = ax1.plot(z.data.numpy(), net(z).data.numpy(), '-', c=c2, lw=2, animated=True, label=r'$h_{\\theta^t}(x)$') # current estimated function\n",
    "    t = ax1.annotate(\"iteration: \"+str(it),(0.4,0.95),xycoords='figure fraction',annotation_clip=False) # add text\n",
    "    if it == 0:\n",
    "        ax1.scatter(x.data.numpy(), y.data.numpy(), color=c1)\n",
    "        if plot_OLS:\n",
    "            ztilde = np.concatenate((np.linspace(-1,1,100),np.ones(100))).reshape(2,100).T\n",
    "            xtilde = np.concatenate((x,np.ones_like(x)),axis=1)\n",
    "            beta,_,_,_ = np.linalg.lstsq(xtilde,y,rcond=None)\n",
    "            ax1.plot(z.data.numpy(), ztilde@beta, '--', dashes=(5, 10), zorder=3, c=c4, lw=1, label=r'$\\langle \\beta^*,x \\rangle$') # plot OLS\n",
    "        if plot_stew:\n",
    "            x_stew = np.array([-1, -0.7, -0.55, -0.4, 0, 0.5, 0.6, 0.7, 1])\n",
    "            y_stew = np.array([0, 0.7, 0.5, 1, 0, 1, 0.6, 0.7, 0])\n",
    "            ytilde = np.interp(z.data.numpy(), x_stew, y_stew)\n",
    "            ax1.plot(z.data.numpy(), ytilde, '--', dashes=(5, 10), zorder=3, c=c4, lw=1, label=r'$h_{\\theta^*}(x)$') # plot OLS\n",
    "        ax1.legend(fontsize=14)\n",
    "    ims.append([im1,t])\n",
    "    \n",
    "ani = animation.ArtistAnimation(fig, ims, interval=100, repeat=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('estimated_function.mp4', fps=10, dpi=120) # save animation as .mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save specific frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/opt/anaconda3/lib/python3.9/site-packages/matplotlib/animation.py:889: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you have outputted the Animation using `plt.show()` or `anim.save()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "del ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(iters, np.ndarray):\n",
    "    iters = iters.tolist()\n",
    "\n",
    "it_to_save = iters[73] # the number is the frame number to save\n",
    "#it_to_save = 0       # the number is the iteration number to save\n",
    "frame = iters.index(it_to_save)\n",
    "save_single_frame(fig, ims, frame, name=\"frame_estim\") # save specific frame of animation as a .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_to_save = iters[-1]\n",
    "frame = iters.index(it_to_save)\n",
    "save_single_frame(fig, ims, frame, name=\"frame_estim_kinks\") # save specific frame of animation as a .pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, lw=3)\n",
    "plt.ylim(ymin=0)\n",
    "#plt.xlim(xmin=0, xmax=20000)\n",
    "plt.ylabel(r'$L(\\theta)$',fontsize=20)\n",
    "plt.xlabel('Iterations', fontsize=20)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_profile.pdf',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
